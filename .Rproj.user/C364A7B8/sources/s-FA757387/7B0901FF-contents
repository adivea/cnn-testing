# Exercise 1
?rnorm
x <- rnorm(1000, mean=10, sd=5)
x1 <- x[x<5]
length(x[x<5])/1000
pnorm(5,10,5)
pnorm(1.96,0,1)


?Distributions
?pnorm
y <- pnorm(5,mean=10, sd=5, lower.tail = TRUE)
y

# Exercise 2
?pgamma
1-pgamma(0.68, 3, scale = 1, lower.tail = FALSE)

?pexp
pexp(10, rate = 0.05, )


# Exercise 3
?pbirthday
pbirthday(15, classes = 365, coincident=2)
pbirthday(25, classes = 687, coincident =2)

set.seed(1)
steps<-rnorm(n=10000,mean=0,sd=1)
distance.from.origin <- cumsum(steps)
plot(distance.from.origin,type='l')
stem(x)

boxplot(x)

#QQ plot

qqnorm(runif(1e5))

# Exercise 4
shells <- read.csv('shells.csv') # import data into dataframe
head(shells)                     # prints out the first few rows
names(shells)                    # names of columns in dataframe
summary(shells)                  # summary of values in dataframe
levels(shells$type)              # levels of categorical variable
?t.test                          # help on the t test function
?wilcox.test                     # help on the wilcoxon function

exposed <- shells$length[shells$type=="exposed"]
sheltered <- shells$length[shells$type=="sheltered"]

m <- mean(exposed - sheltered)
m

t.test(length~type, shells, alternative="less")  # gives you Welch t.test not Student {make var.equal= TRUE if you want Student}
str(t.test(length~type, shells))
wilcox.test(length~type, shells)  # gives you Welch t.test not Student {make var.equal= TRUE if you want Student}


# MC randomization test related to Exercise 4
exposed   <- shells$length[shells$type=='exposed']
sheltered <- shells$length[shells$type=='sheltered']
diff      <- mean(exposed) - mean(sheltered)
combined  <- c(exposed, sheltered)
rand_r    <- c() # vector to store differences in means
for(i in 1:1e4) {
  #random permutation of full set of samples
  sample_r    <- sample(combined, size = length(combined), replace = FALSE)
  # allocation of samples to exposed group
  exposed_r   <- sample_r[1:length(exposed)]
  # allocation of samples to sheltered group
  sheltered_r <- sample_r[(length(exposed)+1):length(sample_r)]
  # difference in means for permuted samples
  rand_r[i]   <- mean(exposed_r) - mean(sheltered_r)
}
hist(rand_r) # plot of differences under null model
diff         # empirically observed difference in means
# p values under different null hypotheses
length(rand_r[abs(diff) <= abs(rand_r)])/1e4

# bootstrap 95% CI for shell data related to Exercise 4
boot_r <- c()          # vector to store bootstrap values
for(i in  1:10000) {   # sets up loop
  # random draw of exposed samples
  exposed_r <- sample(exposed, size = length(exposed), replace = TRUE)
  # random draw of sheltered samples
  sheltered_r <- sample(sheltered, size = length(sheltered), replace = TRUE)
  # bootstrap difference in means
  boot_r[i] <- mean(exposed_r) - mean(sheltered_r)
}
hist(boot_r)
boot_r <- sort(boot_r)  # sort values from smallest to largest
boot_r[250]             # lower 95% CI
boot_r[9750]            # upper 95% CI


# Exercise 5
tapply(shells$length,shells$type,mean)                # mean for each group
delta <- diff(tapply(shells$length,shells$type,mean)) # difference in means
tapply(shells$length,shells$type,var)                 # var for each group
s <- sqrt(mean(tapply(shells$length,shells$type,var)))     # pooled standard deviation estimate (equal sample sizes)
?power.t.test
power.t.test(n=51,delta=diff,sd=s,power=NULL)
power.t.test(n=NULL,delta=diff,sd=s,power=0.95)


# Exercise 6
gala <- read.table('gala.txt',header=TRUE,row.names=1)
head(gala)      # prints out the first few rows
names(gala)     # names of columns in dataframe
summary(gala)   # summary of values in dataframe



mounds <- read.csv()
# Exercise 7

plot(gala)
gala.model <- lm(Species~Area,gala)  # continuous measure of Area makes this possible as linear regression

plot(Species~Area, gala)

str(gala.model);summary(gala.model);anova(gala.model)

gala.model$coefficients[2]
residuals(gala.model)
cooks.distance(gala.model)

plot(gala.model)
# first plot shows non linear replationship btw Species number and island area. We want a flat line at zero.
# we want everything on a diagonal, but there is substantial skew
# we want flat line, the lowest possible sqrt mean error. here residual is blown up due to nonnormality, heteroscedaticity, 
# cook's distance: again none of the assumptions are correct for linear regression, data needs to be transformed, and refitted.

# Exercise 8
gala$log.nearest <- log10(gala$Nearest)
gala$log.Area <- log10(gala$Area)  #create a new column with transformed data
gala$log.Species <- log10(gala$Species)  #create a new column with transformed SPecies (to prevent it getting bunched up in top corner)
model.new <- lm(gala$log.Species~gala$log.Area)
plot(model.new)
plot(gala$log.Area, gala$log.Species, xlab="hello world")
abline(model.new)

summary(model.new) # does area significantly explain variance in species number 


anova(model.new) # does area explain variance from 0 in species?
confint(model.new, level=0.99)

gala$log.Elevation <- log10(gala$Elevation)
gala$log.Scruz <- log10(gala$Scruz)
gala$log.Adjacent <- log10(gala$Adjacent)
gala$log.Nearest <- log10(gala$Nearest)

gala.model <- lm(gala$log.Species~gala$log.Area+gala$log.Elevation)

summary(gala.model)
lm1 <- lm(gala$log.Species~gala$log.Area)
lm2 <- lm(gala$log.Species~gala$log.Elevation)  # crude surrogate for area if area is not in the picture

#test of fitness  - to evaluate whether you get a serious determining variable
anova(lm2)

?drop1
drop1(gala.model, test="Chisq")





  
# Linear models
head(gala)
ffa.lm = lm(log.Species~log.Area+log.Elevation+log.Nearest+log.Scruz+log.Adjacent, data=gala)
null.lm = lm(log.Species~1, gala)
#then do FS
?step()
step(ffa.lm, direction='backward')  


#Experiment (scary story about data dredging procedures)
# unless motivated by theory, data need to be take with a grain of salt
# buyer beware when using stepwise procedures
junk.data <- matrix(rnorm(100*11),nrow-100, ncol=11)
junk.data <- data.frame(junk.data);names(junk.data)



#Factor variable type
ssize <- sample(0:20, 40, replace=TRUE)
is.factor(ssize)
ssize.f <- factor(ssize, labels = c("s","m","l"))
is.factor(ssize.f)


# Exercise 9
mangroves <- read.csv('mangroves.csv')

summary(mangroves)
mangroves$treatmentf <- factor(mangroves$treatment, labels=c("Bare","Fake","Live1","Live2"))
lm1 <- lm(growth~treatment, mangroves)
lm2 <- lm(growth~treatmentf, mangroves)
anova(lm1,lm2)

step(lm1, direction="backward")
step(lm2, direction="backward")

## Compare the two models
AIC(lm1, lm2)  # first model comes out as better, although it is garbage! 
# we don't worry about the absolute magnitude of AIC, but relative difference matters between different models/variables


?Tukey  #posthoc text
TukeyHSD(aov(lm2))  # provides comparison in mean growth rate between group 1 and group 2

# group 2:1 difference is not significant (P>0.05)
# group 4:1 has the largest difference - probably the bare root has least growth

anova(lm2)
anova(lm1)
# at least 1 group differs from the others significantly, overall significance
summary(lm2)
# bare control level is missing, serves as control here
# Estimate column tells us how much each of the treatments differ from control (add the number to the top control numebr)

# Readjust the levels to look 
relevel(mangroves$treatmentf,ref="4")

#Contrasts
contrasts(mangroves$treatmentf)
# all contrasts are orthogonal, but are independent; 
# preplanned contrasts allows to evaluate very targeted hypotheses and gives more statistical power
# if you have 4 levels you will have three evalautions



# Two way ANOVA

rats <- read.csv('rats.csv')
head(rats)
str(rats)
plot(time~treat+poison, data=rats)
interaction.plot(rats$treat,rats$poison,rats$time)
interaction.plot(rats$poison,rats$treat, rats$time)
# last plot > vertices correspond to a mean; interaction between lines means that efficacy treatment depends on the poison used.
# non-parallel lines signal that we should expect to see significant difference in two-way anova
# interaction effects

g <- lm(time~poison*treat,rats)
anova(g)
qqnorm(g$res)
qqline(g$res)
# very fat tails in the qq charts

# lets express this as a rate, rather than time > do an inverse transform 
#only after residuals are normalized can we turn to our ANOVA
g <- lm(1/time~poison*treat,rats)
anova(g)
qqnorm(g$res)
qqline(g$res)


# Exercise 10 
# Do treatments vary in effectiveness?
# Yes, treat comes as p value < 0.05 So does Poison so poisons have different toxicity.
# But the effect of treatment is not changing on the basis of poison we are applying.


#   ANCOVA
# mixing categorical and continuous data
# mashed crab dioxin concentration variability between sites

#TEQ total equivalent dose of dioxin as a function of time is what matters
# are teh slope for two sites different?

dioxin <- read.csv("dioxin.csv")
head(dioxin)

dioxin.model <- lm(logTEQ~Site*Year, dioxin)
summary(dioxin.model)
# p-value for Siteb is more than 0.05, so Site is not significant , we can do a new model assuming the slope is the same


dioxin.model2 <- lm(logTEQ~Site+Year, dioxin)
summary(dioxin.model2)
# Year matters, as dioxin increases every year! 170 coefficient

plot(dioxin$Year,dioxin$logTEQ,  xlab='year',ylab='log(TEQ)',type='n')
points(dioxin$Year[dioxin$Site=='a'], dioxin$logTEQ[dioxin$Site=='a'])
points(dioxin$Year[dioxin$Site=='b'],dioxin$logTEQ[dioxin$Site=='b'],pch=15,col="red")

   
   
 #### ANOVA   # Anova() function in car package

## GLM
# for non normal variables such as counts, binary values. ONLY for exactly TWO categories.
# it is called generalized because we use p for success, using linear function
# get log (p/1-p) = a+bx

# Poisson regression
# Negative binomial regressions - for counts 
# exponential and gamma - for non=negative continuous variables

# Logistic regression
binary <- read.csv("binary.csv")
summary(binary)
head(binary)
binary$f.rank <- factor(binary$rank)

m1 <- glm(admit~gre+gpa+f.rank,family= binomial(logit), binary)
?glm
summary(m1)
# two people with same GRE/GPA but different rank schools have lower chance of getting in if from lower rank school
# estimate indicates lower probability of low rank schools at fixed GPA and GRE

m2 <- glm(admit~gpa*f.rank, family = binomial(logit), binary) # families poisson, gamma
summary(m2)
m3 <- glm(admit~gpa+f.rank, family = binomial(logit), binary)
# format significance could be assessed by two models, one for interaction, one for main effect, and an anova
anova(m2,m3,test="Chisq")
AIC(m2,m3)

# we are searching for maximum likelihood in our fitted models

# Exercise 13
m4 <- glm(admit~gpa+gre+f.rank, family=binomial(logit), binary)
drop1(m4, test="Chisq")  # look which of the terms have the lowest AIC >> gre
?drop1

m5 <- glm(admit~gpa+f.rank, family=binomial(logit), binary)
drop1(m5, test="Chisq")
summary(m5)  # rank of schools

range(binary$gpa)

x <- seq(2.26,4, 0.01)

